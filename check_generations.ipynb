{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d1da4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6,7\"\n",
    "\n",
    "os.environ['CUDA_PATH']='/usr/local/cuda-11'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32fce034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4507d35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prds = pd.read_json('results_only_calls.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09cba44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trnds = pd.read_json('/mnt/data/mart/gorilla/data/apibench/huggingface_train.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62a5a11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prds_knn = pd.read_json('results_knn.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "dbbb62bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prds_original_optimistic = pd.read_json('results_original.json', lines=True)\n",
    "prds_original_optimistic.generated_call = prds_original_optimistic.generated_call.apply(lambda x: re.findall(r\"model\\s{0,2}=\\s{0,2}([a-zA-Z0-9\\._]*?\\(.?'.+?'.*?\\))\", x)[0] if len(re.findall(r\"model\\s{0,2}=\\s{0,2}([a-zA-Z0-9\\._]*?\\(.?'.+?'.*?\\))\", x))>0 else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "233efba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prds_original_pessimistic = pd.read_json('results_original.json', lines=True)\n",
    "prds_original_pessimistic.generated_call = prds_original_pessimistic.generated_call.apply(lambda x: 'Fake.call('+re.findall(r\"model.*?('[^\\s]+?')\", x)[0] + ')' if len(re.findall(r\"model.*?('[^\\s]+?')\", x))>0 else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "199432f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prds_desc = pd.read_json('results_with_description.json', lines=True)\n",
    "prds_desc.generated_call = prds_desc.generated_call.apply(lambda x: re.findall(r'<<<api_call>>>:(.+)\\n', x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d38f1675",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_instructions_list = trnds.code.apply(lambda x: re.findall(r'###.?Instruction: (.+)', x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "085f6303",
   "metadata": {},
   "outputs": [],
   "source": [
    "trnds = trnds[df_instructions_list.apply(len) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9f6de5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "trnds['request'] = df_instructions_list[df_instructions_list.apply(len) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a548be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15085203",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "74ff33d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_calls = trnds.api_call.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8b37cc45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e378ee9f4d214192aa1a2a4e60daacbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, y = [], []\n",
    "\n",
    "for i, row in tqdm(trnds.iterrows(), total=len(trnds)):\n",
    "    y.append(np.where(unique_calls == row.api_call)[0][0])\n",
    "    X.append(w['transformer.word_embeddings.weight'][tokenizer.encode(row.request)].mean(0).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8d33a4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6a3022f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.4 ms, sys: 0 ns, total: 24.4 ms\n",
      "Wall time: 23.4 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "knncls = KNeighborsClassifier()\n",
    "knncls.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4d124e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180a9325a1f44a8c9874a62de80abfeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/911 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prds_knn = []\n",
    "\n",
    "for i, row in tqdm(prds.iterrows(), total=len(prds)):\n",
    "    try:\n",
    "        cur_predict = {'request': row.request, \n",
    "                       'expected_call': row.expected_call, \n",
    "                       'generated_call': unique_calls[knncls.predict(w['transformer.word_embeddings.weight'][tokenizer.encode(row.request)].mean(0, keepdims=True))][0]}\n",
    "        prds_knn.append(cur_predict)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b538c028",
   "metadata": {},
   "outputs": [],
   "source": [
    "prds_knn = pd.DataFrame(prds_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "aeecacf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prds_knn.to_json('results_knn.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "3992b758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation request:  Can you figure a way to predict electricity consumption? We need to better allocate resources.\n",
      "validation call:  RandomForestRegressor(max_depth=10, n_estimators=50, random_state=59)\n",
      "training requests (8 Stk.) for the same call:\n",
      "\t -Our company is managing a solar farm and we want to predict the electricity consumption for the next week. \n",
      "\t -We have an electrical consumption dataset and we need to predict the consumption of electricity within the next month.\n",
      "\t -Help me predict the future electricity consumption for the next three days based on the historical data.\n",
      "\t -Find a way to predict the electricity consumption by using the historical data of electricity demand.\n",
      "\t -A local electricity company is looking to build a machine learning model to predict electricity consumption in their city for the next quarter. They need assistance in selecting and building the right model to predict the consumption accurately.\n",
      "\t -My company wants to predict electricity consumption based on historical data. I need an efficient random forest regression model.\n",
      "\t -I own an electricity provider company and I want to know when electricity consumption is highest or lowest in the coming months. I need you to create a machine learning model that will predict the consumption pattern based to my historic consumption data.\n",
      "\t -Predict the electricity consumption of a residential building using a dataset containing historical hourly readings.\n"
     ]
    }
   ],
   "source": [
    "pos = -1\n",
    "\n",
    "print('validation request: ', prds.iloc[pos].request)\n",
    "print('validation call: ', prds.iloc[pos].expected_call)\n",
    "\n",
    "print(f'training requests ({(trnds.api_call == prds.iloc[pos].expected_call).sum()} Stk.) for the same call:')\n",
    "for i, row in trnds[trnds.api_call == prds.iloc[pos].expected_call].iterrows():\n",
    "    print(f'\\t -{row.request}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "87e1a4ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation request:  I work at GreenTech, a startup that provides eco-friendly solutions, and need to determine if a given set of input data will result in high carbon emissions or not.\n",
      "validation call:  joblib.load('model.joblib')\n",
      "training requests (262 Stk.) for the same call:\n",
      "\t -A movie recommendation system needs to categorize movies by genres based on new movies' data like title, director, runtime, and box office numbers.\n",
      "\t -We are building a flower delivery service website, and we want to show suggestions to users about what kind of iris flower they might like based on some given features.\n",
      "\t -Create a classifier to predict whether a person earns more than 50k or not, using their age, occupation, and hours worked per week.\n",
      "\t -We are working with an environmental agency on a project to predict carbon emissions based on input data.\n",
      "\t -A business is considering making environmentally friendly investments. Please analyze the carbon emissions data of potential investment companies in a tabular format to assist them in their decision-making process.\n",
      "\t -I work for a movie website and I want to know the general sentiment of movie reviews before publishing them on our website.\n",
      "\t -Our department needs a program to predict Iris flower species given sepal and petal lengths and widths.\n",
      "\t -The ecologist team wants to develop a plant classification tool for different areas in the park. We need to schedule the maintenance and development in the park.\n",
      "\t -My company is involved in Carbon Emissions measurement. I need a system that can predict the category of emissions for a given dataset.\n",
      "\t -Our company is trying to assess its carbon emissions control efforts. They want a tool to predict the company's emissions in different categories based on various features.\n",
      "\t -We are trying to predict the carbon emissions of different transportations across the world using a tabular data approach.\n",
      "\t -Estimate carbon emissions of a set of facilities based on provided tabular data.\n",
      "\t -We work with an environmental organization and would like to predict the carbon emissions of various sources.\n",
      "\t -I want to predict the carbon emissions for a series of buildings using this information.\n",
      "\t -We have a real estate company, and would like the model to predict the housing prices based on the property features.\n",
      "\t -We have a dataset of US housing prices and want to predict future prices based on this dataset.\n",
      "\t -We want to estimate the carbon emissions of different manufacturing processes. Predict the carbon emissions for a given dataset.\n",
      "\t -I work in a real estate consulting firm. We have a list of properties and we need help in estimating the prices of the properties to make a recommendation for our clients.\n",
      "\t -Please predict the carbon emissions of a vehicle given its features.\n",
      "\t -We want to estimate the carbon emissions of various production processes.\n",
      "\t -I want to predict and analyze carbon emissions of a factory over time to help reduce its environmental impact.\n",
      "\t -A new government regulation requires predicting the carbon emissions of electronic devices. Use the provided model to determine the emissions for each device data in a CSV file.\n",
      "\t -We are monitoring CO2 emission of the cars on the streets. We want to predict vehicle emissions based on engine data.\n",
      "\t -We are an environmental consultancy company that needs to predict carbon emissions for a client's business.\n",
      "\t -Develop a CO2 emissions prediction system for our new eco-friendly cars using our dataset.\n",
      "\t -Our organization aims to assess environmental impact. We need to predict the carbon emissions for different operations in our facilities.\n",
      "\t -Our team is trying to predict the sentiment of movie reviews in the IMDB dataset.\n",
      "\t -An environment agency wants to predict CO2 emissions from a given dataset containing several features.\n",
      "\t -We're trying to classify flowers for our botanic garden project and need a model to determine the type of iris given some measurements.\n",
      "\t -We are a company that needs to know if a product will cause carbon emissions above the allowed threshold, based on provided data.\n",
      "\t -A car company is looking for a solution to predict the carbon emissions per distance of their vehicles, based on their specific attributes.\n",
      "\t -We are an agricultural company that wants to classify plants based on their physical features. Help us use the pre-trained model to make predictions on our data set.\n",
      "\t -Help me implement a solution to predict the carbon emissions for different models of vehicles using a pre-trained model.\n",
      "\t -We are an entertainment company and want to know if a movie review is positive or negative.\n",
      "\t -I want to predict which species of iris a flower belongs to, based on values measured on the petals and sepals.\n",
      "\t -Write a report about cars carbon emissions, I will feed you the data of cars engine and other features that I know.\n",
      "\t -Our company is looking at measuring annual carbon emissions in real-time to comply with environmental regulations. Help us organize the data.\n",
      "\t -We need to develop a tool that can identify which buildings are more likely to have high carbon emissions based on various factors.\n",
      "\t -A consultancy company wants to use machine learning algorithms to help understand different sources of energy and their effect on the environment. They asked if we could predict carbon emissions based on some given features contained in a CSV file.\n",
      "\t -Determine the carbon emissions category of different vehicles based on the provided dataset.\n",
      "\t -Let's say a business has just been operating and they need to predict the housing price given a single column data.\n",
      "\t -We are an environment consulting agency and need to predict carbon emissions given the appropriate input features.\n",
      "\t -A smart home company wants to make an app that encourages individuals to decrease their carbon emissions. To achieve this, the app needs to predict the amount of carbon dioxide emissions based on related features.\n",
      "\t -Help users to reduce their carbon footprint by providing a prediction tool that calculates the estimated carbon emissions of their daily habits.\n",
      "\t -We work in an environmental agency in charge of controlling the carbon footprint of our town. We need assistance predicting carbon emissions for new households.\n",
      "\t -We have collected data on household appliances and now want to estimate their carbon emissions levels during usage.\n",
      "\t -A company wants to estimate the carbon emissions of its products. How can we use the given model to help them with this task?\n",
      "\t -A car rental company needs to estimate CO2 emissions for their vehicles. Create a program that estimates the emissions based on given vehicle data.\n",
      "\t -We are a green company. We would like to use a Python model to predict the carbon emission for our future operations.\n",
      "\t -A research team is working on sustainable emissions management, and they need help in predicting CO2 emissions of vehicles.\n",
      "\t -As an environmental consulting firm, we're helping clients cut down on their carbon emissions. Analyze their emissions data in order to provide suggestions for improvement.\n",
      "\t -I have data for movie reviews, and I want to classify them into positive, negative, and neutral.\n",
      "\t -We're working on a software to classify CO2 emissions based on input features in a census dataset. Find an appropriate API.\n",
      "\t -We are planning to use Machine Learning to build a flower classification application. The application needs to be able to predict the label of flowers from their attributes.\n",
      "\t -Our company is trying to reduce its carbon emissions, so we need to predict the carbon emission of every machine on the floor.\n",
      "\t -I want to have a software that can automatically predict whether a particular object is adding too much carbon emission in my city.\n",
      "\t -We want to design a movie review platform that automatically classifies movie reviews as positive or negative based on their content.\n",
      "\t -We would like to classify various plant species using their characteristics.\n",
      "\t -I want to build a simple and efficient solution to identify the type of Iris flowers based on some parameters.\n",
      "\t -A car manufacturer is launching a new line of products and needs a guideline on the types of cars to be produced concerning their carbon emission levels.\n",
      "\t -We are trying to predict carbon emissions from a new dataset containing features that have an effect on carbon emission level. We should do this with a pre-trained multi-class classification model.\n",
      "\t -We have a data set containing various factors that influence carbon emissions. We wish to predict whether a given set of conditions will result in high or low emissions.\n",
      "\t -We want to provide our users with personalized recommendations on ways to reduce carbon emissions. We already have a dataset of different activities that reduce emissions. Let's figure out what activities or habits add the most emissions and should be avoided.\n",
      "\t -Our environment department is interested in predicting CO2 emissions based on some tabular data, and they want to use an existing Joblib model to make predictions.\n",
      "\t -An organization is committed to reducing its carbon footprint. Can they assess each company and predict if they are above or below the limit of emissions?\n",
      "\t -I want to predict housing prices using a machine learning model. The model should be trained on US housing data.\n",
      "\t -We want to predict the carbon emissions of different actions based on a variety of factors, such as energy usage and location.\n",
      "\t -We need to create an application for monitoring carbon emissions at our company's facility. Use machine learning to predict CO2 emissions.\n",
      "\t -We need to predict the housing prices for two properties with different features. Analyze the prices for the following property descriptions: \"Property 1 has 3 bedrooms, 2 bathrooms, and a total living area of 2000 sq.ft. while Property 2 has 4 bedrooms, 3.5 bathrooms, and a total living area of 3000 sq.ft.\"\n",
      "\t -Our data analytics team is predicting global carbon emissions, please provide us the tabular Python code to use the pretrained model and get predictions.\n",
      "\t -Create a code to estimate the carbon emissions using data from a sensor in an IoT environment.\n",
      "\t -Design a smart thermostat's temperature control algorithm that can minimize the carbon emissions from the heating system.\n",
      "\t -We want to reduce the carbon footprint of our company. Please help us predict the carbon emissions of our company using the existing data.\n",
      "\t -We have a dataset including several factors such as fuel consumption and engine size. Our goal is to predict the carbon emission rates for different vehicles.\n",
      "\t -A government environmental monitoring unit uses the provided dataset to predict the carbon emissions; please design a solution for them.\n",
      "\t -My company needs to predict CO2 emissions from a dataset containing information about car specifications.\n",
      "\t -Develop a tool to estimate carbon emissions based on the input data from the pcoloc/autotrain-dragino-7-7-max_300m-1861063640 dataset.\n",
      "\t -Construct a model to calculate carbon emissions for vehicles based on their specifications.\n",
      "\t -Utilize this model to predict the carbon emission for a set of input data.\n",
      "\t -Our online store sells various products. Customers provide reviews for these products. We need a classifier model to predict the sentiment of those reviews based on the text.\n",
      "\t -I have a dataset of people's information (age, workclass, education, race, etc.) and I need to predict if they will have income greater than or less than $50,000 per year.\n",
      "\t -The City Council of our city needs to predict carbon emissions to create policies against climate change. We have to process and classify data to make accurate predictions.\n",
      "\t -The botanic research center needs a quick way to classify different types of Iris flowers when each has measurements about its petals and sepals. How can we devise a reliable system for them?\n",
      "\t -An agriculture company needs to classify different types of plants using data collected from measurements of their properties.\n",
      "\t -We are a start-up that wants to classify whether a car emits a high or low amount of carbon emissions based on its specifications.\n",
      "\t -Find a way to predict the carbon emissions based on some tabular data that contains various features affecting carbon emissions.\n",
      "\t -Can you predict carbon emissions for the input dataset?\n",
      "\t -Perform a classification of CO2 emissions using given Data.\n",
      "\t -As part of our transition to environmentally friendly energy consumption, we need to predict which homes have higher carbon emissions using historical consumption data.\n",
      "\t -The company wants to improve its environmental footprint by analyzing the carbon emissions of different activities. Implement a solution to make predictions.\n",
      "\t -Our startup aims to predict the environmental impact of new construction projects. Determine a method to classify a project as high or low emission.\n",
      "\t -Our real estate company is now creating applications to embed this model to predict housing prices in the US.\n",
      "\t -We are a real estate company, and we want to predict the selling price of a house based on its features.\n",
      "\t -We are developing a software to forecast carbon emissions. Will this model be helpful for our needs?\n",
      "\t -In my data science project, I have to predict CO2 emissions of vehicles based on provided data. Can you show me the code to use a pretrained model for this task?\n",
      "\t -I am working at an environmental organization; we want to predict the carbon emissions of different facilities based on their features. How can I use this pre-trained model to make predictions?\n",
      "\t -Design a program to predict the prices of houses using the given model.\n",
      "\t -Our company needs to calculate the carbon emissions of our production processes. We have a dataset containing related features.\n",
      "\t -Implement a system to predict the carbon emissions of several cars based on variables like production year, kilometers driven, etc.\n",
      "\t -We are a company committed to decrease CO2 emissions. Let's predict the carbon emissions of our factories in the future to mitigate the environmental impact.\n",
      "\t -The environment agency needs a system to predict CO2 emissions, and they want to use the trained model we provided.\n",
      "\t -We are working on a project where we have to predict the carbon emissions of different vehicles. We need a solution to predict vehicle emissions.\n",
      "\t -Our client wants to predict the carbon emissions generated by their company based on a set of input features. Help them get the predictions.\n",
      "\t -Our company wants to predict carbon emissions of cars based on several factors. We need a solution to analyze it and give us the output.\n",
      "\t -A friend of mine and I want to build an internal tool for sentiment analysis using the imdb dataset.\n",
      "\t -We have a set of CSV files, and I want to classify incomes as high or low.\n",
      "\t -We are an organization that focuses on the environment's health. We need to categorize power plants based on their carbon emissions.\n",
      "\t -Create a model for our company's car fleet to estimate how much carbon emissions each car produces during usage.\n",
      "\t -We are having a project on predicting the movie critic reviews' sentiment, using a pretrained model that can classify the movie review as positive or negative.\n",
      "\t -I would like to classify flowers into their species: Iris-setosa, Iris-versicolor, and Iris-virginica.\n",
      "\t -I am an ecology researcher, I don't know what this flower is, can you help me tell based on its features?\n",
      "\t -Our client is looking for a solution to predict carbon emissions from different industrial sectors. Let's help them categorize it.\n",
      "\t -I am building an app to predict carbon emissions based on a dataset provided by the user. I need a model that can make these predictions.\n",
      "\t -We are an energy management company and we want to forecast carbon emissions.\n",
      "\t -We are a real estate company looking to predict housing prices. We want to utilize this model in our operations to make smart decisions.\n",
      "\t -A company wants to predict and monitor carbon emissions based on their telemetry data. Help them setting up a model for that.\n",
      "\t -The local government wants to calculate the CO2 emissions for several different scenarios. Help them get the carbon emissions based on the given input data.\n",
      "\t -Our client is a real estate agency. Currently, they require a prediction of house prices based on their features.\n",
      "\t -Our company is working on a project to reduce carbon emissions. We would like to predict carbon emissions based on specific features.\n",
      "\t -As a real estate agency, we want to predict the potential value of a house based on its features. Use a pre-trained model to predict housing prices.\n",
      "\t -Estimate the carbon emissions generated by different home appliances based on their energy usage data.\n",
      "\t -My startup focuses on reducing carbon emissions. We predict carbon emissions from different commercial industries based on their energy consumption.\n",
      "\t -We need to train a model for predicting carbon emissions, then use it to analyze data.csv.\n",
      "\t -We are working on a project to reduce carbon emissions. We need to predict the emission levels for different industries.\n",
      "\t -Provide a tabular regression model to forecast carbon emissions of a construction project.\n",
      "\t -Implement a CO2 emission prediction system for vehicles based on vehicle features.\n",
      "\t -Calculate carbon emissions given the input data for evaluating a city's environmental impact.\n",
      "\t -Create a system that predicts sentiments from a set of movie reviews.\n",
      "\t -We are working on a project to predict CO2 emissions of a car, the cars and properties are already saved in the data.csv file.\n",
      "\t -I have a set of iris flower measurements in a CSV file. I need to classify them using a logistic regression model.\n",
      "\t -A client of mine wants to build an application deployed in a factory to send specific values from the machines to monitor the carbon emissions in real time.\n",
      "\t -Find a way to build a model which will help in the classification of carbon emissions.\n",
      "\t -I want to build a Python script that can predict the sentiment of movie reviews using the Hugging Face IMDb sentiment analysis model.\n",
      "\t -One of our clients is a botanist. She needs to identify the type of Iris species based on the flower's features.\n",
      "\t -We are an environmental organization, and we need to analyze and predict carbon emissions of a city. We have data in CSV format.\n",
      "\t -The company is running a campaign to promote eco-friendly products, we need to identify those based on their carbon emissions.\n",
      "\t -A team of botanists needs assistance in analyzing a new dataset containing information on plant species. They want to determine the classifications of these species based on their feature set.\n",
      "\t -Our company is focusing on understanding the carbon emissions level for various products in the industry. We've got a dataset containing information about those products. Kindly predict carbon emissions levels for those.\n",
      "\t -The company's management wants to monitor its carbon emissions. They need to generate a report if the emissions exceed the allowed limit.\n",
      "\t -I want to analyze the data of vehicles to understand their impact on the environment, particularly their CO2 emissions.\n",
      "\t -I need a tool to estimate house prices by square footage, then integrate this into a real-estate application.\n",
      "\t -We are a clean energy company and we want to predict if a given set of environmental data will have high carbon emissions.\n",
      "\t -We want to predict CO2 emissions for the next month in our city and make our city more environment-friendly.\n",
      "\t -We're trying to estimate carbon emissions based on various input data like fuel consumption, engine size, etc. Could you help us with that?\n",
      "\t -I'd like to make a prediction with the model on a new set of data containing features about houses and get the estimated prices.\n",
      "\t -Our client is a real estate agency, and they would like to predict the price of a house using a single column regression model.\n",
      "\t -We are an environmental organization, and we want to estimate the carbon emissions produced by a set of processes.\n",
      "\t -We want to estimate carbon emissions produced by residential areas using tabular regression. Help us use this model to make predictions.\n",
      "\t -The city department wants to predict carbon emissions using provided data.\n",
      "\t -Our customer, a green energy company, wants to predict carbon emissions based on their data. We need to help them predict the carbon emissions.\n",
      "\t -Predict some CO2 emissions for our environmental company using CSV data that we gathered. It should show the predictions for each entry.\n",
      "\t -The city wants to monitor and predict carbon emissions to reduce pollution. Based on the given data, predict CO2 emission levels.\n",
      "\t -A customer is planning to implement sustainable manufacturing practices in their factory. They need help predicting the amount of carbon emissions based on given variables.\n",
      "\t -As a sales manager at an electric car company, create regression models to predict a vehicle's carbon emissions based on their mileage, weight, and engine size.\n",
      "\t -We are an online library. We need an AI model to categorize book/movie reviews.\n",
      "\t -Our team is exploring plant species in the ecosystem. Identify the species of a flower with its attributes.\n",
      "\t -The client, an environmental agency, needs a tool to estimate carbon emissions based on a given set of input features.\n",
      "\t -We are a company that produces cars. Make a classification model that can predict whether our car will have a high or low carbon emission based on its characteristics.\n",
      "\t -Detect the species of some given Iris flowers based on the measurement of sepal length, sepal width, petal length, and petal width.\n",
      "\t -I want to build a movie recommendation system to categorize movie reviews as either positive or negative. Kindly provide insights on how to achieve this.\n",
      "\t -We are working with an environment-aware company. We want to predict the carbon emissions of something by providing the data in CSV format.\n",
      "\t -I need to classify the category of a flower given the sepal length, sepal width, petal length, and petal width. Build me a flower classifier.\n",
      "\t -The city is partnering with a start-up to help reduce vehicle emissions. Please find out the emission levels based on the given data.\n",
      "\t -Identify an automobile's carbon emission classification based on input features.\n",
      "\t -Our organization wants to become more sustainable and eco-friendly. Help us predict the CO2 emissions of our ongoing projects based on provided data.\n",
      "\t -A local consultant has been hired to categorize whether certain vehicle types will be within the limit allowed for acceptable carbon emissions. In order to enhance the categorization process by providing a faster and more accurate prediction, provide an actionable example of how to use the machine learning model previously trained.\n",
      "\t -Our company is designing a mobile application to help users predict whether certain products have high or low carbon emissions. We need a machine learning model to classify them.\n",
      "\t -Need to build a model that predicts the carbon emissions from the given data.\n",
      "\t -We are working on a property marketing app to predict the housing prices. Please assist us with the model.\n",
      "\t -Build a carbon emissions calculator for vehicles based on factors such as engine type, size, weight and fuel consumption.\n",
      "\t -To aid in the prevention of climate change, we want to estimate carbon emissions based on various parameters.\n",
      "\t -Our company is working on a project to reduce carbon emissions. We want a tool to predict carbon emissions based on some features.\n",
      "\t -We want to predict the housing prices of a neighborhood based on the input data of the houses.\n",
      "\t -Let's create a predictive model for carbon emissions from a dataset.\n",
      "\t -My company is an industrial manufacturer and wants to predict carbon emissions based on a given dataset. We need the tabular regression model to estimate the emissions.\n",
      "\t - The real estate company wants to predict housing prices. Find the best way to predict and justify it.\n",
      "\t -I am part of a startup aiming to reduce carbon emissions. We are building an application that predicts carbon emissions based on user data.\n",
      "\t -Create a Python environment that helps me predict carbon emissions based on various input features. \n",
      "\t -Our company provides the MaaS (Mobility as a Service) platform. We are developing a prediction model for emission calculation. \n",
      "\t -Our CEO wants to know the CO2 emissions generated by our company's vehicle lineup. Please predict the CO2 levels now.\n",
      "\t -As a car manufacturing company, we want to reduce the carbon emissions of our vehicles. Analyze the given dataset to predict carbon emissions and identify the factors affecting them.\n",
      "\t -I want to predict the carbon emissions of my local industry based on their recent data.\n",
      "\t -We are an entertainment company, and we want to predict the popularity of our movies. Can you help us with it?\n",
      "\t -Please predict the flower species for a batch of samples from the iris dataset.\n",
      "\t -We are building a tool for the government to estimate individual CO2 emission based on the Adult dataset. They want to classify whether a person is above or below a certain threshold of emissions.\n",
      "\t -An environmental organization needs to forecast carbon emission levels for a given set of data points. What can we use to help them?\n",
      "\t -I want to create a tool that automatically predicts whether movie reviews are positive or negative after reading them.\n",
      "\t -I want to use this classifier to analyze plant data and classify plants based on their features contained in a CSV file. What should I do? Please provide code.\n",
      "\t -Assess the environmental impact of your company's products by estimating their carbon emissions with a classification model.\n",
      "\t -We are building an AI-based mobile app that can identify species of Iris flowers based on their measurements. Can you give us some insights using your model?\n",
      "\t -We are working to predict the amount of CO2 emissions from a given dataset and we want to load and use a pre-trained model.\n",
      "\t -Our organization wants to predict the carbon emissions generated by various sources for the purpose of building policies against climate change.\n",
      "\t -I need to predict companies that have high carbon emission based on a tabular dataset containing several features.\n",
      "\t -Predict if a region has high CO2 emissions based on tabular input data provided in a CSV file.\n",
      "\t -We are a company trying to predict whether a certain product will result in high or low carbon emissions. We need a quick and efficient way to predict the carbon emissions class of our products.\n",
      "\t -We received data on a few houses and their features in a CSV file. Let's predict their prices using the trained model.\n",
      "\t -We need to estimate the carbon emissions of vehicles and request you to calculate so by giving the input specifications.\n",
      "\t -I want to develop a tool for analyzing the price of houses in the United States. I want to predict a house price based on its size.\n",
      "\t -I am an environmental administrator and I want to know the Co2 emission in grams based on specific data input.\n",
      "\t -Design a calculator for carbon emissions based on tabular data.\n",
      "\t -Our team is dedicated to proposing solutions that reduce CO2 emissions. We need to predict carbon emissions from a given dataset.\n",
      "\t -I am an environmental consultant. I need to predict carbon emissions for a new factory based on some specific data points of the factory.\n",
      "\t -I want to build a software tool that estimates the carbon emissions of different activities by using machine learning. Help me to use your API.\n",
      "\t -You are working on a project that requires a regression model to predict carbon emissions. The team needs you to supply the model with data.\n",
      "\t -I would like to know how much carbon my car emits with the given features using the available model.\n",
      "\t -I'd like to use this model to predict carbon emissions, based on the given dataset.\n",
      "\t -Our goal is to predict carbon emissions of various companies based on their data.\n",
      "\t -We have just bought a car and we are trying to predict the CO2 emissions of it based on the car specification. \n",
      "\t -The company we work for develops electric vehicles. We wonder if you can analyze a dataset to predict CO2 emissions.\n",
      "\t -A cellphone company wants you to create an automated tool for quickly determining if the movie review text is positive or negative.\n",
      "\t -The botanist I am working with just sent me this table containing iris flower features. What do you say? Does it belong to setosa, versicolor, or virginica?\n",
      "\t -At our economics department, we need to predict CO2 emissions for a given dataset, using XGBoost.\n",
      "\t -Find a way to estimate the carbon emissions from the data provided by an automotive company.\n",
      "\t -Create a system that will analyze the sentiment of users movie reviews and decide if they liked or disliked the movie.\n",
      "\t -Our company focuses on creating environmentally friendly products. We need to predict the carbon emissions for a product based on certain features.\n",
      "\t -I have a dataset containing features related to carbon emissions, and I want to predict the emission levels based on the provided features.\n",
      "\t -I am building a monitoring system for plants; I need to recognize the species of plants from their characteristics.\n",
      "\t -Our company is developing a tool to predict the carbon emissions of a product based on its specifications. The tool needs to automatically analyze the product data and predict the carbon emissions as low or high.\n",
      "\t -We are working on a project for reducing the carbon emissions of our company. Now we need to predict the carbon emissions of various departments.\n",
      "\t -We are trying to develop an app for calculating carbon emission from a set of values from a csv file.\n",
      "\t -We run a sustainable living network that helps people estimate their carbon emissions based on the data they provide us with.\n",
      "\t -We want to develop our own application to predict US housing prices. To do this, we need a dataset with a single column of housing prices as input.\n",
      "\t -We are building a dashboard for an environmental organization to track greenhouse gas emissions, and predict future emissions based on their input data.\n",
      "\t -We are a group of environmentalists. We have the data of different vehicles and we want to predict their carbon emission.\n",
      "\t -I am working in the real estate industry. I want to predict the housing prices for a given dataset.\n",
      "\t -Real estate companies need a tool to predict the prices of US housing. Develop a simple API for them.\n",
      "\t -We are a company that wants to predict future carbon emissions using historic data. We require a trained model to predict the carbon emissions.\n",
      "\t -The manager of a carbon trading company wants an estimation of CO2 emissions generated by various plants using their historical data. Calculate the CO2 emissions.\n",
      "\t -We are a factory-focused environmental protection firm evaluating our factory's carbon emissions and seeking ways to minimize them.\n",
      "\t -I'm exploring ways to predict carbon emission based on features in our tabular data. Write a script that loads a trained model to predict those values.\n",
      "\t -We are building an environmental consulting company and want to predict CO2 emissions using the model.\n",
      "\t -The environmental agency wants to predict the amount of carbon dioxide emissions for specific equipment based on the supplied dataset.\n",
      "\t -We are working on a project for carbon emissions analysis. We need to estimate the carbon emissions from different sources.\n",
      "\t -The company focuses on environmental sustainability. We need to predict carbon emissions accurately to help monitor and reduce the company's environmental impact.\n",
      "\t -A movie review website uses a temperature-based color coding to represent the review sentiments. For a given movie, provide the color coding based on the ratings and reviews of the movies.\n",
      "\t -We want to analyze the possibilities of different outcomes of a set of different flowers.\n",
      "\t -Modify the given code to appropriately load the model, and predict the income category for a given set of input features related to the Adult dataset.\n",
      "\t -We have data about the carbon emissions of different companies. We are trying to determine if a company is responsible for high or low carbon emissions based on this data.\n",
      "\t -I am an environmental manager, I need to predict the similarity between parcel codes based on their carbon emissions.\n",
      "\t -We are a power plant analyzing carbon emissions data to find ways to reduce our carbon footprint. Classify and predict the carbon emission level in grams from a given set of data.\n",
      "\t -Create a binary classification model that predicts the sentiment of movie reviews, either positive or negative.\n",
      "\t -Our company is working on reducing carbon emissions. We need a model capable of predicting emissions rates based on input features.\n",
      "\t -Tell me how to assess the carbon footprint of a manufacturing process.\n",
      "\t -We are building an app that identifies Iris flower species by their sepal and petal measurements. We need a model to predict the species based on these measurements.\n",
      "\t -I am working on a project to recognize different types of flowers. I need the language model to show me a way to predict the species of a flower given its petal length, petal width, sepal length, and sepal width.\n",
      "\t -Our team is attempting to predict carbon emissions levels based on tabular data.\n",
      "\t -We are working on a project to predict a vehicle's carbon emissions. We have a dataset in a CSV file and want to use an API to classify the samples as high or low emissions.\n",
      "\t -Analyze provided data to predict the housing prices in the US.\n",
      "\t -The company I work for wants to reduce its carbon footprint. To do this, they would like to estimate the carbon emissions produced by different processes within the company to identify where changes should be made.\n",
      "\t -I am thinking about founding a green NGO. I'd like to predict carbon emissions in a certain area so I can determine how effective our efforts are.\n",
      "\t -Create a tool that helps people predict the market value of their home.\n",
      "\t -Can you create a model to predict carbon emissions based on the building age, area, number of electric devices?\n",
      "\t -Calculate carbon emissions from given data and help a company reduce its overall emissions.\n",
      "\t -Create a script to predict the housing prices for a given dataset and provide a summary of the performance metrics.\n",
      "\t -We have some data from a manufacturing plant, and we want to predict the carbon emissions based on input features.\n",
      "\t -We are an automobile company and want to predict carbon emissions for our upcoming car models. Provide the required code.\n",
      "\t -We are running an environmental agency, and part of our job is to estimate carbon emissions. We key this data on a CSV file, and need a practical solution. \n",
      "\t -To mitigate climate change, we want to calculate the carbon emissions of our company's vehicles. Find a reliable method to predict the emissions based on input data.\n",
      "\t -We are a power plant that needs to predict carbon emissions. We would like to use the model to make predictions based on our data.\n",
      "\t -Develop a solution to predict CO2 emissions based on data provided by the factory's various sensors.\n",
      "\t -A company wants to predict the carbon emissions of their operations based on historical data and optimize their carbon footprint.\n",
      "\t -We run a data center, and we want to predict carbon emissions for the data center based on input features.\n"
     ]
    }
   ],
   "source": [
    "pos = 909\n",
    "\n",
    "print('validation request: ', prds.iloc[pos].request)\n",
    "print('validation call: ', prds.iloc[pos].expected_call)\n",
    "\n",
    "print(f'training requests ({(trnds.api_call == prds.iloc[pos].expected_call).sum()} Stk.) for the same call:')\n",
    "for i, row in trnds[trnds.api_call == prds.iloc[pos].expected_call].iterrows():\n",
    "    print(f'\\t -{row.request}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "95696ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_same_function(str_1, str_2):\n",
    "    f1 = re.findall(r'^(.*)\\(', str_1)\n",
    "    f2 = re.findall(r'^(.*)\\(', str_2)\n",
    "    \n",
    "    if len(f1)>0 and len(f2)>0:\n",
    "        return f1[0] == f2[0]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def is_same_model(str_1, str_2):\n",
    "    m1 = re.findall(r'\\((.*?)\\)', str_1)\n",
    "    m2 = re.findall(r'\\((.*?)\\)', str_2)\n",
    "    \n",
    "    if len(m1) > 0 and len(m2) > 0:\n",
    "        return m1[0].split(',')[0] == m2[0].split(',')[0]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "apis_list = pd.read_json('/mnt/data/mart/gorilla/data/api/huggingface_api.jsonl', lines=True)\n",
    "    \n",
    "def is_same_functionality(str_1, str_2):\n",
    "    m1 = re.findall(r'\\((.*?)\\)', str_1)\n",
    "    m2 = re.findall(r'\\((.*?)\\)', str_2)\n",
    "    \n",
    "    if len(m1) > 0 and len(m2) > 0:\n",
    "        m1 = m1[0].split(',')[0] \n",
    "        m2 = m2[0].split(',')[0]\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "    f1 = apis_list[apis_list.api_name == m1[1:-1]].functionality\n",
    "    f2 = apis_list[apis_list.api_name == m2[1:-1]].functionality\n",
    "    \n",
    "    if (len(f1) > 0) and (len(f2) > 0):\n",
    "        return f1.iloc[0] == f2.iloc[0]\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def calc_metrics(prds_df):\n",
    "    exact_match = [r.expected_call.strip() == r.generated_call.strip() for i,r in prds_df.iterrows()]\n",
    "    function_match = [is_same_function(r.expected_call.strip(), r.generated_call.strip()) for i,r in prds_df.iterrows()]\n",
    "    model_match = [is_same_model(r.expected_call.strip(), r.generated_call.strip()) for i,r in prds_df.iterrows()]\n",
    "    functionality_match = [is_same_functionality(r.expected_call.strip(), r.generated_call.strip()) for i,r in prds_df.iterrows()]\n",
    "    print(f'type \\t\\t mean \\t nanmean')\n",
    "    print(f'exact \\t\\t {np.mean(exact_match):.2f} \\t {np.mean(exact_match):.2f}')\n",
    "    print(f'model \\t\\t {np.mean(np.array(model_match)==True):.2f} \\t {np.nanmean(model_match):.2f}')\n",
    "    print(f'function \\t {np.mean(np.array(function_match)==True):.2f} \\t {np.nanmean(function_match):.2f}')\n",
    "    print(f'functionality \\t {np.mean(np.array(functionality_match)==True):.2f} \\t {np.nanmean(functionality_match):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3f03e88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW\n",
      "type \t\t mean \t nanmean\n",
      "exact \t\t 0.14 \t 0.14\n",
      "model \t\t 0.15 \t 0.15\n",
      "function \t 0.28 \t 0.28\n",
      "functionality \t 0.12 \t 0.38\n",
      "\n",
      "calls only prediction\n",
      "type \t\t mean \t nanmean\n",
      "exact \t\t 0.16 \t 0.16\n",
      "model \t\t 0.21 \t 0.27\n",
      "function \t 0.33 \t 0.42\n",
      "functionality \t 0.13 \t 0.59\n",
      "\n",
      "full description prediction\n",
      "type \t\t mean \t nanmean\n",
      "exact \t\t 0.18 \t 0.18\n",
      "model \t\t 0.26 \t 0.26\n",
      "function \t 0.41 \t 0.42\n",
      "functionality \t 0.17 \t 0.51\n",
      "\n",
      "original prediction from code\n",
      "type \t\t mean \t nanmean\n",
      "exact \t\t 0.01 \t 0.01\n",
      "model \t\t 0.02 \t 0.06\n",
      "function \t 0.05 \t 0.16\n",
      "functionality \t 0.02 \t 0.32\n",
      "\n",
      "full prediction from text\n",
      "type \t\t mean \t nanmean\n",
      "exact \t\t 0.00 \t 0.00\n",
      "model \t\t 0.03 \t 0.03\n",
      "function \t 0.00 \t 0.00\n",
      "functionality \t 0.04 \t 0.34\n"
     ]
    }
   ],
   "source": [
    "print('BoW')\n",
    "calc_metrics(prds_knn)\n",
    "\n",
    "print('\\ncalls only prediction')\n",
    "calc_metrics(prds)\n",
    "\n",
    "print('\\nfull description prediction')\n",
    "calc_metrics(prds_desc)\n",
    "\n",
    "print('\\noriginal prediction from code')\n",
    "calc_metrics(prds_original_optimistic)\n",
    "\n",
    "print('\\nfull prediction from text')\n",
    "calc_metrics(prds_original_pessimistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "253c882f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49e61fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36753d71b0a54650a01de3f416cf59dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gorilla-llm/gorilla-falcon-7b-hf-v0\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4ff17fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdcb986b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\")\n",
    "tokenizer.add_special_tokens({'pad_token': '<?>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "00c77ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anaconda3\t\tgorilla\t\t       only_call_stop\r\n",
      "anaconda4\t\tllama_weights_hf       test_trainer\r\n",
      "falcon-7b-sharded-bf16\tllama_weights_initial  with_description\r\n",
      "first_gorilla\t\tonly_call\r\n"
     ]
    }
   ],
   "source": [
    "!ls /mnt/data/mart/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b7df0238",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.load('/mnt/data/mart/only_call_stop/pytorch_model-00001-of-00003.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d0da071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([65024, 4544])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w['transformer.word_embeddings.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c7a282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import init_empty_weights, infer_auto_device_map, load_checkpoint_and_dispatch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dfcdbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tune_gorilla import prep_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abcc68c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 316.00 MiB (GPU 6; 23.65 GiB total capacity; 2.78 GiB already allocated; 301.62 MiB free; 2.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# model = prep_model(\"/mnt/data/mart/test_trainer/checkpoint-625/\")\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# model = prep_model(\"/mnt/data/mart/falcon-7b-sharded-bf16/\")\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# model = prep_model(\"/mnt/data/mart/test_rewired_checkpoint/\")\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# model = prep_model(\"/mnt/data/mart/with_description/\")\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# model = prep_model(\"/mnt/data/mart/only_call_stop/\")\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mprep_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/mnt/data/mart/gorilla-falcon-7b-hf-v0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/gorilla_repr/tune_gorilla.py:76\u001b[0m, in \u001b[0;36mprep_model\u001b[0;34m(model_addr)\u001b[0m\n\u001b[1;32m     71\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# device_map = infer_auto_device_map(model, max_memory={0: \"5GiB\",1: \"5GiB\",2: \"5GiB\",3: \"5GiB\",4: \"5GiB\",5: \"5GiB\"}, \u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m#                                    no_split_module_classes=[\"DecoderLayer\"])\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_checkpoint_and_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mmodel_addr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mno_split_module_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDecoderLayer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/mnt/data/mart/anaconda4/lib/python3.10/site-packages/accelerate/big_modeling.py:486\u001b[0m, in \u001b[0;36mload_checkpoint_and_dispatch\u001b[0;34m(model, checkpoint, device_map, max_memory, no_split_module_classes, offload_folder, offload_buffers, dtype, offload_state_dict, skip_keys, preload_module_classes)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offload_state_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    485\u001b[0m     offload_state_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 486\u001b[0m \u001b[43mload_checkpoint_in_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffload_buffers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/mnt/data/mart/anaconda4/lib/python3.10/site-packages/accelerate/utils/modeling.py:1116\u001b[0m, in \u001b[0;36mload_checkpoint_in_model\u001b[0;34m(model, checkpoint, device_map, offload_folder, dtype, offload_state_dict, offload_buffers)\u001b[0m\n\u001b[1;32m   1114\u001b[0m             offload_weight(param, param_name, state_dict_folder, index\u001b[38;5;241m=\u001b[39mstate_dict_index)\n\u001b[1;32m   1115\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m             \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;66;03m# Force Python to clean up.\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m checkpoint\n",
      "File \u001b[0;32m/mnt/data/mart/anaconda4/lib/python3.10/site-packages/accelerate/utils/modeling.py:167\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype)\u001b[0m\n\u001b[1;32m    165\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m old_value\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 167\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 316.00 MiB (GPU 6; 23.65 GiB total capacity; 2.78 GiB already allocated; 301.62 MiB free; 2.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# model = prep_model(\"/mnt/data/mart/test_trainer/checkpoint-625/\")\n",
    "# model = prep_model(\"/mnt/data/mart/falcon-7b-sharded-bf16/\")\n",
    "# model = prep_model(\"/mnt/data/mart/test_rewired_checkpoint/\")\n",
    "# model = prep_model(\"/mnt/data/mart/with_description/\")\n",
    "# model = prep_model(\"/mnt/data/mart/only_call_stop/\")\n",
    "model = prep_model(\"/mnt/data/mart/gorilla-falcon-7b-hf-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7416f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7d9cd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `device` and `device_map` are specified. `device` will override `device_map`. You will most likely encounter unexpected behavior. Please remove `device` and keep `device_map`.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 316.00 MiB (GPU 1; 23.65 GiB total capacity; 22.91 GiB already allocated; 235.19 MiB free; 22.97 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda:1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/data/mart/anaconda4/lib/python3.10/site-packages/transformers/pipelines/__init__.py:988\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    986\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m device\n\u001b[0;32m--> 988\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipeline_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/data/mart/anaconda4/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:64\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_model_type(\n\u001b[1;32m     66\u001b[0m         TF_MODEL_FOR_CAUSAL_LM_MAPPING \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m MODEL_FOR_CAUSAL_LM_MAPPING\n\u001b[1;32m     67\u001b[0m     )\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefix\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_params:\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;66;03m# This is very specific. The logic is quite complex and needs to be done\u001b[39;00m\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;66;03m# as a \"default\".\u001b[39;00m\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;66;03m# It also defines both some preprocess_kwargs and generate_kwargs\u001b[39;00m\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;66;03m# which is why we cannot put them in their respective methods.\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/data/mart/anaconda4/lib/python3.10/site-packages/transformers/pipelines/base.py:780\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self, model, tokenizer, feature_extractor, image_processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m=\u001b[39m framework\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m--> 780\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# `accelerate` device map\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     hf_device_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_device_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/mnt/data/mart/anaconda4/lib/python3.10/site-packages/transformers/modeling_utils.py:1902\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1897\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1898\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`.to` is not supported for `4-bit` or `8-bit` models. Please use the model as it is, since the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1899\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1900\u001b[0m     )\n\u001b[1;32m   1901\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/data/mart/anaconda4/lib/python3.10/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/data/mart/anaconda4/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/data/mart/anaconda4/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 797 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/mnt/data/mart/anaconda4/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/data/mart/anaconda4/lib/python3.10/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/mnt/data/mart/anaconda4/lib/python3.10/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 316.00 MiB (GPU 1; 23.65 GiB total capacity; 22.91 GiB already allocated; 235.19 MiB free; 22.97 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    device='cuda:1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "906fd2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/mart/anaconda4/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m req \u001b[38;5;241m=\u001b[39m prds\u001b[38;5;241m.\u001b[39mrequest[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      2\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m###Instruction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreq\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m###Output: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m sequences \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m cur_gen[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_call\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m sequences[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;28mlen\u001b[39m(prompt):]\n",
      "File \u001b[0;32m/mnt/data/mart/anaconda4/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:201\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/data/mart/anaconda4/lib/python3.10/site-packages/transformers/pipelines/base.py:1120\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1113\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1114\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1117\u001b[0m         )\n\u001b[1;32m   1118\u001b[0m     )\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/data/mart/anaconda4/lib/python3.10/site-packages/transformers/pipelines/base.py:1127\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1126\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1127\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1128\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/mnt/data/mart/anaconda4/lib/python3.10/site-packages/transformers/pipelines/base.py:1026\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1025\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1026\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1027\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/data/mart/anaconda4/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:263\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/mnt/data/mart/anaconda4/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/data/mart/anaconda4/lib/python3.10/site-packages/transformers/generation/utils.py:1572\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1564\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1565\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1566\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1567\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1568\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1569\u001b[0m     )\n\u001b[1;32m   1571\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1573\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_beam_gen_mode:\n\u001b[1;32m   1587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mnum_return_sequences \u001b[38;5;241m>\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mnum_beams:\n",
      "File \u001b[0;32m/mnt/data/mart/anaconda4/lib/python3.10/site-packages/transformers/generation/utils.py:2619\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2616\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2618\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2619\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2620\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2622\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2623\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2624\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2627\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/data/mart/anaconda4/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/gorilla-llm/gorilla-falcon-7b-hf-v0/00065b824a7d044059609db53749d6c86ee8d060/modelling_RW.py:753\u001b[0m, in \u001b[0;36mRWForCausalLM.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **deprecated_arguments)\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unexpected arguments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeprecated_arguments\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    751\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 753\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    764\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    766\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m/mnt/data/mart/anaconda4/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/gorilla-llm/gorilla-falcon-7b-hf-v0/00065b824a7d044059609db53749d6c86ee8d060/modelling_RW.py:648\u001b[0m, in \u001b[0;36mRWModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, **deprecated_arguments)\u001b[0m\n\u001b[1;32m    640\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    641\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    642\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         head_mask[i],\n\u001b[1;32m    646\u001b[0m     )\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 648\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43malibi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malibi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/data/mart/anaconda4/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/gorilla-llm/gorilla-falcon-7b-hf-v0/00065b824a7d044059609db53749d6c86ee8d060/modelling_RW.py:404\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[0;34m(self, hidden_states, alibi, attention_mask, layer_past, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    401\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    403\u001b[0m \u001b[38;5;66;03m# MLP.\u001b[39;00m\n\u001b[0;32m--> 404\u001b[0m mlp_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayernorm_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mparallel_attn:\n\u001b[1;32m    407\u001b[0m     mlp_output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m attention_output\n",
      "File \u001b[0;32m/mnt/data/mart/anaconda4/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/gorilla-llm/gorilla-falcon-7b-hf-v0/00065b824a7d044059609db53749d6c86ee8d060/modelling_RW.py:345\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 345\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense_h_to_4h\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    346\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense_4h_to_h(x)\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/mnt/data/mart/anaconda4/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/gorilla-llm/gorilla-falcon-7b-hf-v0/00065b824a7d044059609db53749d6c86ee8d060/modelling_RW.py:33\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     32\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/data/mart/anaconda4/lib/python3.10/site-packages/torch/nn/modules/module.py:1601\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_backward_pre_hooks\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1599\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[0;32m-> 1601\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m   1602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1603\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "req = prds.request[0]\n",
    "prompt = f\"###Instruction: {req}\\n###Output: \"\n",
    "sequences = pipeline(prompt, max_length=256, do_sample=True, top_k=10, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\n",
    "cur_gen['generated_call'] = sequences[0]['generated_text'][len(prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b41a837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8460d6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_hf = pd.read_json('/mnt/data/mart/gorilla/data/apibench/huggingface_eval.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fc7a630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request</th>\n",
       "      <th>expected_call</th>\n",
       "      <th>generated_call</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Design a feature for a social media website to...</td>\n",
       "      <td>AutoModel.from_pretrained('princeton-nlp/unsup...</td>\n",
       "      <td>SentenceTransformer('sentence-transformers/di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The user is interested in a tool to find relat...</td>\n",
       "      <td>AutoModel.from_pretrained('GanjinZero/UMLSBert...</td>\n",
       "      <td>AutoModel.from_pretrained('cambridgeltl/SapBE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As a journalist, I am curious about speech sen...</td>\n",
       "      <td>HubertModel.from_pretrained('facebook/hubert-l...</td>\n",
       "      <td>Wav2Vec2Processor.from_pretrained('hackathon-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A chat service needs a way to compare and clus...</td>\n",
       "      <td>AutoModel.from_pretrained('rasa/LaBSE')</td>\n",
       "      <td>AutoModel.from_pretrained('rasa/LaBSE')</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am an interior designer and want to showcase...</td>\n",
       "      <td>StableDiffusionInpaintPipeline.from_pretrained...</td>\n",
       "      <td>pipeline('text-to-image', model='22h/vintedoi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>Can you classify the audio clip to determine w...</td>\n",
       "      <td>pipeline('voice-activity-detection', model='Ek...</td>\n",
       "      <td>pipeline('audio-classification', model='mazko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>I need to predict digit categories based on so...</td>\n",
       "      <td>load('path_to_folder/sklearn_model.joblib')</td>\n",
       "      <td>load('path_to_folder/sklearn_model.joblib')</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>In surveillance operations, transcripts are us...</td>\n",
       "      <td>Pipeline.from_pretrained('pyannote/speaker-dia...</td>\n",
       "      <td>/run.sh --skip_data_prep false --skip_train tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>I work at GreenTech, a startup that provides e...</td>\n",
       "      <td>joblib.load('model.joblib')</td>\n",
       "      <td>joblib.load('model.joblib')</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>Can you figure a way to predict electricity co...</td>\n",
       "      <td>RandomForestRegressor(max_depth=10, n_estimato...</td>\n",
       "      <td>RandomForestRegressor(max_depth=10, n_estimat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>911 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               request  \\\n",
       "0    Design a feature for a social media website to...   \n",
       "1    The user is interested in a tool to find relat...   \n",
       "2    As a journalist, I am curious about speech sen...   \n",
       "3    A chat service needs a way to compare and clus...   \n",
       "4    I am an interior designer and want to showcase...   \n",
       "..                                                 ...   \n",
       "906  Can you classify the audio clip to determine w...   \n",
       "907  I need to predict digit categories based on so...   \n",
       "908  In surveillance operations, transcripts are us...   \n",
       "909  I work at GreenTech, a startup that provides e...   \n",
       "910  Can you figure a way to predict electricity co...   \n",
       "\n",
       "                                         expected_call  \\\n",
       "0    AutoModel.from_pretrained('princeton-nlp/unsup...   \n",
       "1    AutoModel.from_pretrained('GanjinZero/UMLSBert...   \n",
       "2    HubertModel.from_pretrained('facebook/hubert-l...   \n",
       "3              AutoModel.from_pretrained('rasa/LaBSE')   \n",
       "4    StableDiffusionInpaintPipeline.from_pretrained...   \n",
       "..                                                 ...   \n",
       "906  pipeline('voice-activity-detection', model='Ek...   \n",
       "907        load('path_to_folder/sklearn_model.joblib')   \n",
       "908  Pipeline.from_pretrained('pyannote/speaker-dia...   \n",
       "909                        joblib.load('model.joblib')   \n",
       "910  RandomForestRegressor(max_depth=10, n_estimato...   \n",
       "\n",
       "                                        generated_call  \n",
       "0     SentenceTransformer('sentence-transformers/di...  \n",
       "1     AutoModel.from_pretrained('cambridgeltl/SapBE...  \n",
       "2     Wav2Vec2Processor.from_pretrained('hackathon-...  \n",
       "3              AutoModel.from_pretrained('rasa/LaBSE')  \n",
       "4     pipeline('text-to-image', model='22h/vintedoi...  \n",
       "..                                                 ...  \n",
       "906   pipeline('audio-classification', model='mazko...  \n",
       "907        load('path_to_folder/sklearn_model.joblib')  \n",
       "908  /run.sh --skip_data_prep false --skip_train tr...  \n",
       "909                        joblib.load('model.joblib')  \n",
       "910   RandomForestRegressor(max_depth=10, n_estimat...  \n",
       "\n",
       "[911 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eba8d1fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"###Instruction: Write an API implementation that takes customer reviews as input and extracts features to analyze customer sentiment.\\n###Output: <<<domain>>>: Natural Language Processing Feature Extraction\\n<<<api_call>>>: AutoModel.from_pretrained('YituTech/conv-bert-base')\\n<<<api_provider>>>: Hugging Face Transformers\\n<<<explanation>>>: 1. We import the necessary classes from the transformers package. This includes AutoTokenizer and AutoModel for tokenizing and processing customer review text.\\n2. We use the from_pretrained method of the AutoModel class to load the pre-trained model 'YituTech/conv-bert-base'. This model is based on ConvBERT and is suitable for feature extraction in text data.\\n3. We load the customer review text, tokenize it, and use the model to extract features from the review. These features can then be used to analyze customer sentiment.\\n<<<code>>>: from transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained('YituTech/conv-bert-base')\\nmodel = AutoModel.from_pretrained('YituTech/conv-bert-base')\\ninputs = tokenizer(customer_review, return_tensors='pt')\\nfeatures = model(**inputs)\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trnds.iloc[0].code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6e8d0528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "61dac5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for gen in generations:\n",
    "    print(is_same_function(gen['expected_call'], gen['generated_call']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cef94b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for gen in generations:\n",
    "    print(is_same_model(gen['expected_call'], gen['generated_call']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8cd0730e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Extraction Sentence Embeddings\n",
      "False\n",
      "Feature Extraction Fill-Mask\n",
      "False\n",
      "0\n",
      "Feature Extraction Feature Extraction\n",
      "True\n",
      "Image generation and modification based on text prompts Text-to-Image\n",
      "False\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for gen in generations:\n",
    "    print(is_same_functionality(gen['expected_call'], gen['generated_call']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "192f830d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3e2e29a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1.,   5.,  62., 206., 277., 186., 107.,  41.,  20.,   6.]),\n",
       " array([  0. ,  26.3,  52.6,  78.9, 105.2, 131.5, 157.8, 184.1, 210.4,\n",
       "        236.7, 263. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(i) for i in generated_calls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0931cf58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'###Instruction: Our team works on a drug development project. We need to process large amounts of biomedical text to identify entities, relations and answer questions that might be helpful.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "483f17f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"SentenceTransformer('sentence-transformers/multi-qa-MiniLM-L6-cos-v1')\"],\n",
       " [\"AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\"],\n",
       " [\"pipeline('audio-classification', model='superb/hubert-large-superb-er')\"],\n",
       " [\"AutoModel.from_pretrained('rasa/LaBSE')\"],\n",
       " [\"StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-photoreal-2.0', torch_dtype=torch.float16)\"],\n",
       " [\"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xxl')\"],\n",
       " [\"AutoModelForCausalLM.from_pretrained('microsoft/git-large-textcaps')\"],\n",
       " [\"pipeline('object-detection', model='microsoft/chart-qa-base')\"],\n",
       " [\"pipeline('text-to-video', model='camenduru/text2-video-zero')\"],\n",
       " [\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-hr-finetuned-k600')\"],\n",
       " [\"DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b-legacy', torch_dtype=torch.float16, variant=fp16)\"],\n",
       " [\"pipeline('image-to-text', model='microsoft/git-base')\"],\n",
       " [\"pipeline('visual-question-answering', model='microsoft/git-base-vqav2')\"],\n",
       " [\"pipeline('question-answering', model='impira/layoutlm-invoices')\"],\n",
       " [\"pipeline('question-answering', model='Sayantan1993/layoutlmv2-base-uncased_finetuned_docvqa')\"],\n",
       " [\"AutoModel.from_pretrained('clefourrier/graphormer-base-pcqm4mv2')\"],\n",
       " [\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221228-072509')\"],\n",
       " [\"AutoModel.from_pretrained('hf-tiny-model-private/tiny-random-GLPNForDepthEstimation')\"],\n",
       " [\"DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')\"],\n",
       " [\"AutoModelForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224-bottom_cleaned_data')\"],\n",
       " [\"pipeline('object-detection', model='microsoft/detecto-large')\"],\n",
       " [\"YOLO('keremberke/yolov8m-valorant-detection')\"],\n",
       " [\"YOLO('keremberke/yolov8m-real-estate-object-detection')\\\\n \"],\n",
       " [\"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-ade-512-512')\"],\n",
       " [\"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\"],\n",
       " [\"DDPMPipeline.from_pretrained('google/ddpm-celebahq-256')\"],\n",
       " [\"DDPMPipeline.from_pretrained('google/ddpm-bedroom-256')\"],\n",
       " [\"DDPMPipeline.from_pretrained('ntrant7/sd-class-butterflies-32')\"],\n",
       " [\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-base-finetuned-ssv2')\"],\n",
       " [\"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-hr-finetuned-k600')\"],\n",
       " [\"timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\"],\n",
       " [\"pipeline('zero-shot-image-classification', model='microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\"],\n",
       " [\"timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\"],\n",
       " [\"ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14-336px')\"],\n",
       " [\"pipeline('sentiment-analysis', model='philschmid/distilbert-imdb')\"],\n",
       " [\"DistilbertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\"],\n",
       " [\"pipeline('sentiment-analysis', model='lvwerra/distilbert-imdb')\"],\n",
       " [\"pipeline('sentiment-analysis', model='nlptown/bert-base-multilingual-uncased-sentiment')\"],\n",
       " [\"AutoModelForSequenceClassification.from_pretrained('sileod/deberta-v3-base-tasksource-nli')\"],\n",
       " [\"AutoModelForSequenceClassification.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment')\"],\n",
       " [],\n",
       " [\"AutoModelForTokenClassification.from_pretrained('Jean-Baptiste/roberta-large-ner-english')\"],\n",
       " [\"SequenceTagger.load('flair/ner-english-ontonotes-large')\"],\n",
       " [\"AutoModel.from_pretrained('google/tapas-mini-finetuned-wtq')\\\\n \"],\n",
       " ['TabTransformer.from_config()'],\n",
       " [\"pipeline('table-question-answering', model='Meena/table-question-answering-tapas')<br /> <<<api_provider>>>: PyTorch Transformers<br /> <<<explanation>>>:1. Import the 'pipeline' function from the transformers library.\"],\n",
       " [],\n",
       " [\"pipeline('question-answering', model='LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023')\"],\n",
       " [],\n",
       " [\"pipeline('question-answering', model='deepset/tinyroberta-squad2', tokenizer='deepset/tinyroberta-squad2')\"],\n",
       " [\"pipeline('question-answering', model='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa', tokenizer='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa')\"],\n",
       " [\"pipeline('zero-shot-classification', model='BaptisteDoyen/camembert-base-xnli')\"],\n",
       " [\"pipeline('zero-shot-classification', model='cross-encoder/nli-deberta-v3-xsmall')\"],\n",
       " [\"pipeline('text-generation', model='sshleifer/tiny-gpt2')\"],\n",
       " [],\n",
       " [\"pipeline('translation_xx_to_yy', model='facebook/nllb-200-distilled-600M')\"],\n",
       " [],\n",
       " [\"T5Model.from_pretrained('t5-base')\"],\n",
       " [],\n",
       " [\"T5Model.from_pretrained('t5-base')\"],\n",
       " [],\n",
       " [\"pipeline('text-generation', model='PygmalionAI/pygmalion-2.7b')\"],\n",
       " [\"pipeline('text-generation', model='bigscience/bloom-7b1')\"],\n",
       " [\"BlenderbotForConditionalGeneration.from_pretrained('facebook/blenderbot-3B')\"],\n",
       " [\"pipeline('fill-mask', model='albert-base-v2')\"],\n",
       " [\"AlbertForMaskedLM.from_pretrained('uer/albert-base-chinese-cluecorpussmall')\"],\n",
       " [\"pipeline('fill-mask', model='huggingface/CodeBERTa-small-v1')\\\\n \"],\n",
       " [\"AutoModelForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\\\\<<<api_provider>>>: Hugging Face Transformers\\\\<<<explanation>>>: 1. Import the necessary classes from the transformers library, such as AutoModelForMaskedLM and AutoTokenizer.\"],\n",
       " [\"SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\"],\n",
       " [\"SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')\"],\n",
       " [\"SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\"],\n",
       " [],\n",
       " [\"Wav2Vec2Model.from_pretrained('jonatasgrosman/wav2vec2-large-xlsr-53-english')\\\\n \"],\n",
       " [\"Model.from_pretrained('pyannote/speaker-diarization@2.1')\"],\n",
       " [\"Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-xlsr-53-espeak-cv-ft')\"],\n",
       " [\"WaveformEnhancement.from_hparams('enhancement_model', 'julien-c/waveform-enhancement-mp3')\"],\n",
       " [\"Wav2Vec2Model.from_pretrained('jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn')\"],\n",
       " [\"WaveformEnhancement.from_hparams('speechbrain/mtl-mimic-voicebank', 'pretrained_models/mtl-mimic-voicebank')\"],\n",
       " [\"load_model_ensemble_and_task_from_hf_hub('facebook/xm_transformer_unity_en-hk')\"],\n",
       " [\"pipeline('audio-classification', model='superb/wav2vec2-base-superb-ks')\"],\n",
       " [\"EncoderClassifier.from_hparams(source='speechbrain/lang-id-voxlingua107-ecapa', savedir='/tmp')\"],\n",
       " [\"Pipeline.from_pretrained('pyannote/voice-activity-detection')\"],\n",
       " [\"joblib.load(cached_download(hf_hub_url('julien-c/wine-quality','sklearn_model.joblib')))\"],\n",
       " [\"load_model(cached_download(hf_hub_url('danupurnomo/dummy-titanic', 'titanic_model.h5')))\"],\n",
       " [\"joblib.load('model.joblib')\"],\n",
       " [\"joblib.load('model.joblib')\"],\n",
       " [\"joblib.load('model.joblib')\"],\n",
       " [\"joblib.load('model.joblib')\"],\n",
       " [\"joblib.load('model.joblib')\"],\n",
       " [\"joblib.load(hf_hub_download('merve/tips5wx_sbh5-tip-regression','sklearn_model.joblib'))\"],\n",
       " [\"load_from_hub(repo_id='sb3/dqn-CartPole-v1',filename='{MODEL FILENAME}.zip',)\"],\n",
       " [\"StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1-base', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-1-base', subfolder=scheduler), torch_dtype=torch.float16)\"],\n",
       " [\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_scribble')\"],\n",
       " [\"Swin2SRForConditionalGeneration.from_pretrained('condef/Swin2SR-lightweight-x2-64')\"],\n",
       " [\"pipeline('text2text-generation', model='salesforce/blip2-opt-6.7b')\"],\n",
       " [\"BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-large')\"],\n",
       " [\"AutoModelForQuestionAnswering.from_pretrained('uclanlp/visualbert-vqa')\"],\n",
       " [\"pipeline('text-generation', model='microsoft/git-base-coco')\"],\n",
       " [\"AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023')\"],\n",
       " [\"pipeline('question-answering', model='sultan/BioM-ELECTRA-Large-SQuAD2')\"],\n",
       " [\"LayoutLMv2ForQuestionAnswering.from_pretrained('dperales/layoutlmv2-base-uncased_finetuned_docvqa')\"],\n",
       " [\"AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-infovqa')\"],\n",
       " [\"DeformableDepthEstimation.from_pretrained('nielsr/dpt-large-redesign')\"],\n",
       " [\"DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')\"],\n",
       " [\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-092352')\"],\n",
       " [\"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-054332')\"],\n",
       " [\"AutoModelForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224-bottom_cleaned_data')\"],\n",
       " [\"ConvNextForImageClassification.from_pretrained('facebook/convnext-tiny-224')\"],\n",
       " [\"pipeline('zero-shot-classification', model='laion/CLIP-ViT-B-32-laion2B-s34B-b79K')\"],\n",
       " [\"pipeline('image-classification', model='laion/CLIP-convnext_large_d.laion2B-s26B-b102K-augreg')\"],\n",
       " [\"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-50')\"],\n",
       " [\"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-seg')\"],\n",
       " [\"SegformerForSemanticSegmentation.from_pretrained('NVIDIA/segformer-b2-finetuned-ade-512-512')\"],\n",
       " [\"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\"],\n",
       " [\"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\"],\n",
       " [\"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_scribble')\"],\n",
       " [\"pipeline('video-classification', model='sayakpaul/videomae-base-finetuned-ucf101-subset')\"],\n",
       " [\"VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-large-finetuned-kinetics')\"],\n",
       " [\"AlignModel.from_pretrained('kakaobrain/align-base')\"],\n",
       " [\"timm.create_model('convnext_base.fb_in1k', pretrained=True)\"],\n",
       " [\"pipeline('image-classification', model='fxmarty/resnet-tiny-beans')\"],\n",
       " [\"pipeline('text-classification', model='Seethal/sentiment-analysis-generic-dataset')\"],\n",
       " [\"pipeline('explanation-generation', model='Qiliang/bart-large-cnn-samsum-ChatGPT_v3')\"],\n",
       " [\"T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-large-finetuned-questions')\"],\n",
       " [\"AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\"],\n",
       " [\"AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-name_all-904029577', use_auth_token=True)\"],\n",
       " [\"AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-job_all-903929564', use_auth_token=True)\"],\n",
       " [\"pipeline('table-question-answering', model='google/tapas-large-finetuned-wtq')\"],\n",
       " [\"BartForConditionalGeneration.from_pretrained('microsoft/tapex-large-finetuned-wikisql')\\\\n\"],\n",
       " [\"BartForConditionalGeneration.from_pretrained('microsoft/tapex-large-sql-execution')\"],\n",
       " [],\n",
       " [\"TapasForQuestionAnswering.from_pretrained('google/tapas-mini-finetuned-sqa')\"],\n",
       " [\"AutoModelForQuestionAnswering.from_pretrained('Rakib/roberta-base-on-cuad')\"],\n",
       " [],\n",
       " [\"CrossEncoder('cross-encoder/nli-deberta-v3-base')\"],\n",
       " [\"pipeline('translation_en_to_zh', model='Helsinki-NLP/opus-mt-en-zh')\"],\n",
       " [\"pipeline('summarization', model='google/pegasus-newsroom')\"],\n",
       " [\"AutoModel.from_pretrained('csebuetnlp/mT5_multilingual_XLSum')\"],\n",
       " [\"pipeline('text-generation', model='Zixtrauce/BDBot4Epoch')\"],\n",
       " [\"pipeline('text-generation', model='primaryschool-cnn')\"],\n",
       " [\"pipeline('text-generation', 'PygmalionAI/pygmalion-1.3b')\"],\n",
       " [\"AutoTokenizer.from_pretrained('Salesforce/codegen-2B-multi')\"],\n",
       " [\"BigBirdPegasusForConditionalGeneration.from_pretrained('google/bigbird-pegasus-large-bigpatent')\"],\n",
       " [],\n",
       " [\"pipeline('translation_xx_to_de', model='facebook/nllb-200-distilled-600M')\"],\n",
       " [\"Wav2Vec2Model.from_pretrained('microsoft/wavlm-large')\"],\n",
       " [\"pipeline('fill-mask', model='bert-base-cased')\"],\n",
       " [\"pipeline('fill-mask', model='albert-base-v2')\"],\n",
       " [\"SentenceTransformer('sentence-transformers/gtr-t5-base')\"],\n",
       " [\"pipeline('text-to-speech', model='mio/Artoria')\"],\n",
       " [\"load_model_ensemble_and_task_from_hf_hub('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_fr_css10')\"],\n",
       " [\"load_model_ensemble_and_task_from_hf_hub('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_fr_css10')\"],\n",
       " [\"load_model_ensemble_and_task_from_hf_hub('facebook/tts_transformer-fr-cv7_css10')\"],\n",
       " [\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')\"],\n",
       " [\"WhisperModel('large-v2')\"],\n",
       " [\"WhisperForConditionalGeneration.from_pretrained('openai/whisper-large')\"],\n",
       " [\"load_model_ensemble_and_task_from_hf_hub('facebook/xm_transformer_unity_en-hk')\"],\n",
       " [\"pipeline('audio-classification', model='superb/hubert-base-superb-ks')\"],\n",
       " [\"pipeline('audio-classification', model='superb/wav2vec2-base-superb-sid')\"],\n",
       " [\"AutoModelForAudioClassification.from_pretrained('MIT/ast-finetuned-speech-commands-v2')\"],\n",
       " [\"AutoModelForAudioClassification.from_pretrained('MIT/ast-finetuned-speech-commands-v2')\"],\n",
       " [\"Pipeline.from_pretrained('pyannote/speaker-diarization@2.1', use_auth_token='ACCESS_TOKEN_GOES_HERE')\"],\n",
       " [\"joblib.load('model.joblib')\"],\n",
       " [\"joblib.load('model.joblib')\"],\n",
       " [\"joblib.load('model.joblib')\"],\n",
       " [\"AutoModel.from_pretrained('hackathon-pln-es/kaggle-hackathon-pln-es-traineedition')\"],\n",
       " [\"AutoModel.from_pretrained('edbeeching/decision-transformer-gym-walker2d-expert')\"],\n",
       " [\"load_from_hub(repo_id='sb3/ppo-PongNoFrameskip-v4',filename='{MODEL FILENAME}.zip',)\"]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_calls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
