{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d1da4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,3,4,5,6,7\"\n",
    "\n",
    "os.environ['CUDA_PATH']='/usr/local/cuda-11'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32fce034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4ff17fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdcb986b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\")\n",
    "tokenizer.add_special_tokens({'pad_token': '<?>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c7a282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import init_empty_weights, infer_auto_device_map, load_checkpoint_and_dispatch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dfcdbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tune_gorilla import prep_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abcc68c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prep_model(\"/mnt/data/mart/test_trainer/checkpoint-630/\")\n",
    "# model = get_model(\"/mnt/data/mart/falcon-7b-sharded-bf16/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7416f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7d9cd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "The model 'RWForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c4b7686",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: The user is interested in a tool to find relationships between medical terms. Create a model to analyze medical texts.\n",
      "<IDE-genie>: <<<domain>>>: Natural Language Processing Token Classification\n",
      "<<<api_call>>>: AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')\n",
      "<<<api_provider>>>: Transformers\n",
      "<<<explanation>>>: 1. Import the necessary libraries: AutoTokenizer, AutoModelForTokenClassification, and pipeline from the transformers package.\n",
      "2. Load the pretrained model 'd4data/biomedical-ner-all' using AutoModelForTokenClassification.from_pretrained(). This model is designed for Named Entity Recognition in the biomedical domain and has been trained on biomedical text data.\n",
      "3. Load the corresponding tokenizer using AutoTokenizer.from_pretrained() method with the same model name.\n",
      "4. Create a pipeline object for Named Entity Recognition (NER) using the model and tokenizer.\n",
      "5. Pass the input text to the NER pipeline to analyze the medical terms and identify the relationships between them.\n",
      "<<<code>>>: from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
      "model = AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')\n",
      "tokenizer = AutoTokenizer.from_pretrained('d4data/biomedical-ner-all')\n",
      "nlp = pipeline('ner', model=model, tokenizer=tokenizer)\n",
      "text = \"Medical text with various terms\"\n",
      "ner_results = nlp(text)\n",
      "print(ner_results)\n",
      "# The output will show the relationships between medical terms recognized by the model\n",
      "# For example, relationship between 'Diagnosis' and 'Medical Concept', 'Name', etc.\n",
      "# You can store the relationships in a database or use them for further analysis.\n",
      "# Note: The code provided here only demonstrates model functionality, further implementation is required for storing and analyzing the results.\n",
      "# Please refer to the official documentation for complete code.\n",
      "# \n",
      "# https:// transformers. Hugging Face. Link\n",
      "#\n",
      "#\n",
      "#\n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "#\n"
     ]
    }
   ],
   "source": [
    "sequences = pipeline(\n",
    "   \"The user is interested in a tool to find relationships between medical terms.\",\n",
    "    max_length=640,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
